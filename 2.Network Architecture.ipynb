{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Network Architecture.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOaW5KjUF0bQSQFmNnuoMaP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"JcoKckHRElp1","colab_type":"code","colab":{}},"source":["# load the data if you need to; if you have already loaded the data, you may comment this cell out\n","# -- DO NOT CHANGE THIS CELL -- #\n","!mkdir /data\n","!wget -P /data/ https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5aea1b91_train-test-data/train-test-data.zip\n","!unzip -n /data/train-test-data.zip -d /data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AaVSotL5EzmJ","colab_type":"code","colab":{}},"source":["# import the usual resources\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","\n","# import utilities to keep workspaces alive during model training\n","from workspace_utils import active_session\n","\n","# watch for any changes in model.py, if it changes, re-load it automatically\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4v0DqgSEzn6","colab_type":"code","colab":{}},"source":["## TODO: Define the Net in models.py\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","## TODO: Once you've define the network, you can instantiate it\n","# one example conv layer has been provided for you\n","from models import Net\n","\n","net = Net()\n","print(net)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xwxbVaREzrL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cTZ29rosL7HR","colab_type":"text"},"source":["### TODO: Define a data transform"]},{"cell_type":"code","metadata":{"id":"ZdpJmtK-Ezsj","colab_type":"code","colab":{}},"source":["from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","\n","# the dataset we created in Notebook 1 is copied in the helper file `data_load.py`\n","from data_load import FacialKeypointsDataset\n","# the transforms we defined in Notebook 1 are in the helper file `data_load.py`\n","from data_load import Rescale, RandomCrop, Normalize, ToTensor\n","\n","\n","## TODO: define the data_transform using transforms.Compose([all tx's, . , .])\n","# order matters! i.e. rescaling should come before a smaller crop\n","data_transform = transforms.Compose([Rescale(250),\n","                                     RandomCrop(224),\n","                                     Normalize(),\n","                                     ToTensor()])\n","\n","\n","# testing that you've defined a transform\n","assert(data_transform is not None), 'Define a data_transform'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_GiU_CfwEzv8","colab_type":"code","colab":{}},"source":["# create the transformed dataset\n","transformed_dataset = FacialKeypointsDataset(csv_file='/data/training_frames_keypoints.csv',\n","                                             root_dir='/data/training/',\n","                                             transform=data_transform)\n","\n","\n","print('Number of images: ', len(transformed_dataset))\n","\n","# iterate through the transformed dataset and print some stats about the first few samples\n","for i in range(4):\n","    sample = transformed_dataset[i]\n","    print(i, sample['image'].size(), sample['keypoints'].size())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKLyyrHhMBpM","colab_type":"text"},"source":["## Batching and loading data"]},{"cell_type":"code","metadata":{"id":"tz7HfRJ7EwTB","colab_type":"code","colab":{}},"source":["# load training data in batches\n","batch_size = 64\n","\n","# train_loader = DataLoader(transformed_dataset, \n","#                           batch_size=batch_size,\n","#                           shuffle=True, \n","#                           num_workers=4)\n","\n","# changed num_workers\n","\n","train_loader = DataLoader(transformed_dataset, \n","                          batch_size=batch_size,\n","                          shuffle=True, \n","                          num_workers=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOp5QPx8EydD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NlbV94tDMOYu","colab_type":"text"},"source":["#### Load in the test dataset\n"]},{"cell_type":"code","metadata":{"id":"IH6qFgMyMO2S","colab_type":"code","colab":{}},"source":["# load in the test data, using the dataset class\n","# AND apply the data_transform you defined above\n","\n","# create the test dataset\n","test_dataset = FacialKeypointsDataset(csv_file='/data/test_frames_keypoints.csv',\n","                                             root_dir='/data/test/',\n","                                             transform=data_transform)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7N1C8qnMTK9","colab_type":"code","colab":{}},"source":["# load test data in batches\n","batch_size = 64\n","\n","test_loader = DataLoader(test_dataset, \n","                          batch_size=batch_size,\n","                          shuffle=True, \n","                          num_workers=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GUXUyFh6MUzR","colab_type":"text"},"source":["## Apply the model on a test sample\n"]},{"cell_type":"code","metadata":{"id":"eo_WcGkhMTNc","colab_type":"code","colab":{}},"source":["# test the model on a batch of test images\n","\n","def net_sample_output():\n","    \n","    # iterate through the test dataset\n","    for i, sample in enumerate(test_loader):\n","        \n","        # get sample data: images and ground truth keypoints\n","        images = sample['image']\n","        key_pts = sample['keypoints']\n","\n","        # convert images to FloatTensors\n","        images = images.type(torch.FloatTensor)\n","\n","        # forward pass to get net output\n","        output_pts = net(images)\n","        \n","        # reshape to batch_size x 68 x 2 pts\n","        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n","        \n","        # break after first image is tested\n","        if i == 0:\n","            return images, output_pts, key_pts\n","            "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZvnIfjR2MTP8","colab_type":"code","colab":{}},"source":["# call the above function\n","# returns: test images, test predicted keypoints, test ground truth keypoints\n","test_images, test_outputs, gt_pts = net_sample_output()\n","\n","# print out the dimensions of the data to see if they make sense\n","print(test_images.data.size())\n","print(test_outputs.data.size())\n","print(gt_pts.size())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OZD-uy1bMhoL","colab_type":"text"},"source":["## Visualize the predicted keypoints\n"]},{"cell_type":"code","metadata":{"id":"3kSEeV7XMb0S","colab_type":"code","colab":{}},"source":["def show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n","    \"\"\"Show image with predicted keypoints\"\"\"\n","    # image is grayscale\n","    plt.imshow(image, cmap='gray')\n","    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n","    # plot ground truth points as green pts\n","    if gt_pts is not None:\n","        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='.', c='g')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_CgCsyVMb22","colab_type":"code","colab":{}},"source":["#### Un-transformation\n","# visualize the output\n","# by default this shows a batch of 10 images\n","def visualize_output(test_images, test_outputs, gt_pts=None, batch_size=10):\n","\n","    for i in range(batch_size):\n","        plt.figure(figsize=(20,10))\n","        ax = plt.subplot(1, batch_size, i+1)\n","\n","        # un-transform the image data\n","        image = test_images[i].data   # get the image from it's Variable wrapper\n","        image = image.numpy()   # convert to numpy array from a Tensor\n","        image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n","\n","        # un-transform the predicted key_pts data\n","        predicted_key_pts = test_outputs[i].data\n","        predicted_key_pts = predicted_key_pts.numpy()\n","        # undo normalization of keypoints  \n","        predicted_key_pts = predicted_key_pts*50.0+100\n","        \n","        # plot ground truth points for comparison, if they exist\n","        ground_truth_pts = None\n","        if gt_pts is not None:\n","            ground_truth_pts = gt_pts[i]         \n","            ground_truth_pts = ground_truth_pts*50.0+100\n","        \n","        # call show_all_keypoints\n","        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts)\n","            \n","        plt.axis('off')\n","\n","    plt.show()\n","    \n","# call it\n","visualize_output(test_images, test_outputs, gt_pts)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"to4nU_IdMuko","colab_type":"text"},"source":["## Training\n"]},{"cell_type":"code","metadata":{"id":"3ADf5_REMrvo","colab_type":"code","colab":{}},"source":["## TODO: Define the loss and optimization\n","import torch.optim as optim\n","\n","criterion = nn.MSELoss()\n","\n","# optimizer = optim.Adam(params = net.parameters(), lr = 0.001)\n","optimizer = optim.Adamax(params = net.parameters(), lr = 0.001)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oxgWyFN0M1K7","colab_type":"text"},"source":["## Training and Initial Observation\n"]},{"cell_type":"code","metadata":{"id":"udozHKFiMz97","colab_type":"code","colab":{}},"source":["def train_net(n_epochs):\n","\n","    # prepare the net for training\n","    net.train()\n","\n","    for epoch in range(n_epochs):  # loop over the dataset multiple times\n","        \n","        running_loss = 0.0\n","\n","        # train on batches of data, assumes you already have train_loader\n","        for batch_i, data in enumerate(train_loader):\n","            # get the input images and their corresponding labels\n","            images = data['image']\n","            key_pts = data['keypoints']\n","\n","            # flatten pts\n","            key_pts = key_pts.view(key_pts.size(0), -1)\n","\n","            # convert variables to floats for regression loss\n","            key_pts = key_pts.type(torch.FloatTensor)\n","            images = images.type(torch.FloatTensor)\n","\n","            # forward pass to get outputs\n","            output_pts = net(images)\n","\n","            # calculate the loss between predicted and target keypoints\n","            loss = criterion(output_pts, key_pts)\n","\n","            # zero the parameter (weight) gradients\n","            optimizer.zero_grad()\n","            \n","            # backward pass to calculate the weight gradients\n","            loss.backward()\n","\n","            # update the weights\n","            optimizer.step()\n","\n","            # print loss statistics\n","            running_loss += loss.item()\n","            if batch_i % 10 == 9:    # print every 10 batches\n","                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/10))\n","                running_loss = 0.0\n","\n","    print('Finished Training')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YPyyPvHM9R2","colab_type":"code","colab":{}},"source":["# train your network\n","n_epochs = 50 # start small, and increase when you've decided on your model structure and hyperparams\n","\n","# this is a Workspaces-specific context manager to keep the connection\n","# alive while training your model, not part of pytorch\n","with active_session():\n","    train_net(n_epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jCP9MhYtNAXL","colab_type":"text"},"source":["## Test data\n"]},{"cell_type":"code","metadata":{"id":"SIjFhtlWM9UT","colab_type":"code","colab":{}},"source":["# get a sample of test data again\n","test_images, test_outputs, gt_pts = net_sample_output()\n","\n","print(test_images.data.size())\n","print(test_outputs.data.size())\n","print(gt_pts.size())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vsqDKa7eM0Af","colab_type":"code","colab":{}},"source":["## TODO: visualize your test output\n","# you can use the same function as before, by un-commenting the line below:\n","\n","visualize_output(test_images, test_outputs, gt_pts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8UnEgyjM0DD","colab_type":"code","colab":{}},"source":["## TODO: change the name to something uniqe for each new model\n","model_dir = 'saved_models/'\n","model_name = 'keypoints_model_1.pt'\n","\n","# after training, save your model parameters in the dir 'saved_models'\n","torch.save(net.state_dict(), model_dir+model_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsyZmRTONLSD","colab_type":"text"},"source":["## Feature Visualization\n"]},{"cell_type":"code","metadata":{"id":"StcZ2G40MryZ","colab_type":"code","colab":{}},"source":["# Get the weights in the first conv layer, \"conv1\"\n","# if necessary, change this to reflect the name of your first conv layer\n","weights1 = net.conv1.weight.data\n","\n","w = weights1.numpy()\n","\n","filter_index = 0\n","\n","print(w[filter_index][0])\n","print(w[filter_index][0].shape)\n","\n","# display the filter weights\n","plt.imshow(w[filter_index][0], cmap='gray')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNkAn2ppNQR9","colab_type":"text"},"source":["## Feature maps\n"]},{"cell_type":"code","metadata":{"id":"N4rCOObeMr1D","colab_type":"code","colab":{}},"source":["##TODO: load in and display any image from the transformed test dataset\n","image = cv2.imread('./images/mona_lisa.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","plt.imshow(image)\n","plt.xticks([]), plt.yticks([])\n","plt.title(\"Orginal Image\")\n","\n","\n","## TODO: Using cv's filter2D function,\n","filtered = cv2.filter2D(image, -1, w[filter_index][0])\n","\n","\n","## apply a specific set of filter weights (like the one displayed above) to the test image\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(121, xticks = [], yticks = [])\n","ax.imshow(filtered)\n","ax.set_title(\"Feature Map\")\n","ax = fig.add_subplot(122, xticks = [], yticks = [])\n","ax.imshow(w[filter_index][0], cmap = 'gray')\n","\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PR2fb3nRMr3k","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4UOovPXXMeGE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}